{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d32f4c9a",
   "metadata": {},
   "source": [
    "# Mathematical Intution :\n",
    "    \n",
    "Suppose we want to generate a music track based on the text prompt: \"upbeat jazz with a saxophone solo\".\n",
    "\n",
    "Text Preprocessing\n",
    "\n",
    "Tokenization: Convert the text prompt into tokens (words or subwords).\n",
    "Example: \"upbeat jazz with a saxophone solo\" ‚Üí [‚Äúupbeat‚Äù, ‚Äújazz‚Äù, ‚Äúwith‚Äù, ‚Äúa‚Äù, ‚Äúsaxophone‚Äù, ‚Äúsolo‚Äù]\n",
    "Embedding: Map these tokens to dense vectors in an embedding space.\n",
    "Mathematical Representation:\n",
    "T = [‚Äúe**upbeat‚Äù, ‚Äúe**jazz‚Äù, ‚Äúe**with‚Äù, ‚Äúe**a‚Äù, ‚Äúe**saxophone‚Äù, ‚Äúe**solo‚Äù]\n",
    "represents the embedding vector for each token.\n",
    "\n",
    "Embedding Vectors (Assuming embeddings are pre-trained):\n",
    "ùëí_upbeat=[0.1,0.2,0.3,‚Ä¶]\n",
    "\n",
    "ùëí_jazz =[0.2,0.1,0.4,‚Ä¶]\n",
    "\n",
    "Contextual Encoding\n",
    "\n",
    "Model Encoding: Use a sequence model (e.g., Transformer) to encode the sequence of embeddings into contextual representations.\n",
    "Example: Apply self-attention mechanisms to understand the relationships between tokens.\n",
    "Mathematical Representation:\n",
    "\n",
    "H=Encoder(T)\n",
    "H is the contextual representation of the input text, capturing the relationships and context.\n",
    "\n",
    "\n",
    "Feature Generation\n",
    "\n",
    "Generation Model: Use a generative model to map the contextual representations to acoustic features.\n",
    "Example: Convert the encoded text representation into a sequence of Mel-spectrogram frames.\n",
    "Mathematical Representation:\n",
    "\n",
    "A=Decoder(H)\n",
    "A represents the generated acoustic features, such as Mel-spectrogram frames, which describe the audio signal in the time-frequency domain.\n",
    "\n",
    "\n",
    "Waveform Synthesis\n",
    "\n",
    "Vocoder: Convert the acoustic features into a waveform (time-domain audio signal).\n",
    "Example: Use a neural vocoder like WaveNet or a traditional vocoder to synthesize audio from the Mel-spectrogram.\n",
    "Mathematical Representation:\n",
    "\n",
    "y=Vocoder(A)\n",
    "y is the final audio waveform.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5d9a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c7bf61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2340ae53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84693e71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3841390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install -U git+https://github.com/facebookresearch/audiocraft#egg=audiocraft\n",
    "!python3 -m pip install -U audiocraft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d529a00",
   "metadata": {},
   "source": [
    "# This code snippet is used to install the audiocraft library, which is a Python package developed by \n",
    "Facebook Research for audio processing and generation. \n",
    "\n",
    "\n",
    "Here's a breakdown of what each command does:\n",
    "\n",
    "(1). !python3 -m pip install -U git+https://github.com/facebookresearch/audiocraft#egg=audiocraft:\n",
    "\n",
    "This command installs the audiocraft library directly from its GitHub repository. \n",
    "The -U flag ensures that the latest version is installed.\n",
    "\n",
    "\n",
    "The git+https://github.com/facebookresearch/audiocraft\n",
    "#egg=audiocraft specifies the GitHub URL where the library's source code is hosted, and #egg=audiocraft helps \n",
    "# pip identify the package name.\n",
    "\n",
    "\n",
    "(2). !python3 -m pip install -U audiocraft:\n",
    "\n",
    "This command ensures that the audiocraft library is installed or updated to the latest version available on PyPI \n",
    "(the Python Package Index).\n",
    "The -U flag ensures that the package is upgraded to the latest version if it's already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4e7b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.models import musicgen\n",
    "from audiocraft.utils.notebook import display_audio\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc61f9ca",
   "metadata": {},
   "source": [
    "# (1). from audiocraft.models import musicgen:\n",
    "\n",
    "This line imports the musicgen module from the audiocraft.models package. \n",
    "The musicgen module likely contains functions or classes related to music generation using deep \n",
    "learning models provided by the audiocraft library.\n",
    "\n",
    "\n",
    "(2). from audiocraft.utils.notebook import display_audio:\n",
    "\n",
    "This line imports the display_audio function from the audiocraft.utils.notebook module. \n",
    "The display_audio function is likely designed to facilitate the playback of audio directly within a \n",
    "Jupyter Notebook environment, making it easier to listen to generated or processed audio.\n",
    "\n",
    "\n",
    "(3). import torch:\n",
    "\n",
    "This line imports the torch library, which is the core library for PyTorch, a popular deep learning framework. \n",
    "PyTorch is often used for building and training deep learning models, and it is likely needed here to support \n",
    "the functionality of the audiocraft \n",
    "library, particularly for tasks like model loading, tensor manipulation, and computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516f0606",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = musicgen.MusicGen.get_pretrained('medium', device='cuda')\n",
    "model.set_generation_params(duration=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd765861",
   "metadata": {},
   "source": [
    "# (1). model = musicgen.MusicGen.get_pretrained('medium', device='cuda'):\n",
    "\n",
    "This line loads a pre-trained MusicGen model from the audiocraft library.\n",
    "get_pretrained('medium') specifies that the medium-sized version of the model should be loaded. The MusicGen model likely comes in different sizes (e.g., small, medium, large), which balance between computational requirements and the quality of the generated music.\n",
    "The device='cuda' argument specifies that the model should run on a CUDA-enabled GPU (if available). Using a GPU accelerates the computation, which is particularly useful for deep learning tasks like music generation.\n",
    "\n",
    "\n",
    "\n",
    "(2). model.set_generation_params(duration=8):\n",
    "\n",
    "This line sets the generation parameters for the model. Specifically, it sets the duration parameter to 8, meaning the model will generate music that is 8 seconds long.\n",
    "set_generation_params is a method of the MusicGen model that allows you to configure various parameters that control the characteristics of the generated music, such as duration, tempo, key, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2248d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.generate([\n",
    "    'crazy EDM, heavy bang',\n",
    "    'classic reggae track with an electronic guitar solo',\n",
    "    'lofi slow bpm electro chill with organic samples',\n",
    "    'rock with saturated guitars, a heavy bass line and crazy drum break and fills.',\n",
    "    'earthy tones, environmentally conscious, ukulele-infused, harmonic, breezy, easygoing, organic instrumentation, gentle grooves',\n",
    "],\n",
    "    progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f20160e",
   "metadata": {},
   "source": [
    "# The progress=True parameter indicates that the generation \n",
    "process will display a progress bar or similar output, allowing the user to see the progress of the music generation.\n",
    "\n",
    "\n",
    "display_audio(res, 32000):\n",
    "\n",
    "The display_audio function is used to play the generated audio within a Jupyter Notebook.\n",
    "The res variable contains the generated music tracks.\n",
    "The 32000 parameter specifies the sample rate at which the audio should be played. In this case, the audio will be played at 32,000 samples per second, which is a common sample rate for good quality audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4335df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f149a091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc31a3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194baaaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a548ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
